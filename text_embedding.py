"""
Text Embedding Module for MVP v3-0
ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆè¨­å‚™å Ã— ãƒã‚§ãƒƒã‚¯é …ç›® â†’ z âˆˆ â„Â¹â°Â²â´ï¼‰

æ©Ÿèƒ½:
  1. ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« (intfloat/multilingual-e5-large, 1024 dim) ã§æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿
  2. OpenAI API (text-embedding-3-large) ã«ã‚ˆã‚‹ä»£æ›¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
  3. FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆè¨­å‚™ã‚«ãƒ†ã‚´ãƒªã®æ„å‘³ç©ºé–“ã®å®‰å®šåŒ–ï¼‰
  4. åŸ‹ã‚è¾¼ã¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼èª­ã¿è¾¼ã¿ï¼ˆå†å®Ÿè¡Œã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰

åˆ©ç”¨ã™ã‚‹è¨­å®š (config_v3.py):
  TEXT_EMBED_BACKEND  : "local" or "openai"
  TEXT_EMBED_LOCAL_MODEL  : HuggingFace ãƒ¢ãƒ‡ãƒ«å
  TEXT_COMBINE_STRATEGY   : "joint" | "concat" | "separate"
  TEXT_EMBED_DIM          : z ã®æ¬¡å…ƒæ•° (1024)
  FAISS_N_NEIGHBORS       : FAISS æ¤œç´¢æ™‚ã® k

ä½¿ã„æ–¹:
  embedder = TextEmbedder()
  z_train = embedder.get_embeddings(train_df)   # (N, 1024)
  embedder.build_faiss_index(z_unique)           # è¨­å‚™ãƒ¦ãƒ‹ãƒ¼ã‚¯åŸ‹ã‚è¾¼ã¿ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
  similar_ids, distances = embedder.faiss_search(query_vec, k=5)
"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

from config_v3 import (
    TEXT_EMBED_BACKEND,
    TEXT_EMBED_LOCAL_MODEL,
    TEXT_EMBED_OPENAI_MODEL,
    OPENAI_EMBED_DIMENSIONS,
    TEXT_EMBED_DIM,
    TEXT_COMBINE_STRATEGY,
    FAISS_N_NEIGHBORS,
    FAISS_USE_GPU,
    TEXT_EMBED_CACHE_DIR,
    FAISS_INDEX_PATH,
    TEXT_EMBED_TRAIN_NPZ,
    TEXT_EMBED_TEST_NPZ,
    TEXT_EMBED_UNIQUE_NPZ,
    TRIPLET_EXTRACT_BATCH_SIZE,
    COLUMNS_V3,
    EQUIP_MASTER_CSV,
    EQUIP_MASTER_ENCODING,
    MASTER_COLS,
)


# =====================================================================
# ãƒã‚¹ã‚¿CSV ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—è¾æ›¸
# =====================================================================

_MASTER_LOOKUP: dict = {}   # (equip_id, check_item_id) -> text
_UNIQUE_TEXTS:  dict = {}   # (equip_id, check_item_id) -> text  (same, used for FAISS)


def load_master_lookup(force: bool = False) -> dict:
    """
    251217_CSV_ãƒã‚§ãƒƒã‚¯é …ç›®_æ•°å€¤çµæœ100ä»¶ä»¥ä¸Š.csv ã‚’èª­ã¿è¾¼ã¿
    (equipment_id, check_item_id) â†’ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ ã®è¾æ›¸ã‚’è¿”ã™ã€‚
    """
    global _MASTER_LOOKUP
    if _MASTER_LOOKUP and not force:
        return _MASTER_LOOKUP

    if not EQUIP_MASTER_CSV.exists():
        print(f"  âš  ãƒã‚¹ã‚¿CSV ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {EQUIP_MASTER_CSV}")
        print("    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return {}

    print(f"  ğŸ“‚ Loading equipment master: {EQUIP_MASTER_CSV.name}")
    mdf = pd.read_csv(EQUIP_MASTER_CSV, encoding=EQUIP_MASTER_ENCODING,
                      usecols=list(MASTER_COLS.values()),
                      dtype={MASTER_COLS['equip_id']: str,
                             MASTER_COLS['check_item_id']: str})

    lookup = {}
    for _, row in mdf.iterrows():
        eid  = str(row[MASTER_COLS['equip_id']]).strip()
        cid  = str(row[MASTER_COLS['check_item_id']]).strip()
        name = str(row.get(MASTER_COLS['equip_name'],   '') or '').strip()
        cat  = str(row.get(MASTER_COLS['equip_category'],'') or '').strip()
        item = str(row.get(MASTER_COLS['check_item_name'],'') or '').strip()
        # multilingual-e5-large æ¨å¥¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        text = f"passage: {cat} {name} {item}".strip()
        lookup[(eid, cid)] = text

    _MASTER_LOOKUP = lookup
    print(f"  âœ“ Lookup built: {len(lookup):,} (equip_id, check_item_id) pairs")
    return lookup


def _build_text_from_ids(equip_id, check_item_id, lookup: dict) -> str:
    """è¨­å‚™ID Ã— ãƒã‚§ãƒƒã‚¯é …ç›®ID ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    key = (str(equip_id).strip(), str(check_item_id).strip())
    return lookup.get(key, f"passage: ç©ºèª¿è¨­å‚™ è¨­å‚™{equip_id} é …ç›®{check_item_id}")


# =====================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =====================================================================


def _normalize(vecs: np.ndarray) -> np.ndarray:
    """L2 æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«å¯¾å¿œï¼‰"""
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    norms = np.where(norms < 1e-12, 1.0, norms)
    return vecs / norms


# =====================================================================
# TextEmbedder ã‚¯ãƒ©ã‚¹
# =====================================================================

class TextEmbedder:
    """
    è¨­å‚™ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ„å‘³çš„åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Example:
        embedder = TextEmbedder()
        z = embedder.get_embeddings(df)   # shape (N, 1024)
        embedder.build_faiss_index(z_unique, labels=unique_ids)
        neighbor_ids, dists = embedder.faiss_search(z[0], k=3)
    """

    def __init__(self, backend: str = TEXT_EMBED_BACKEND):
        self.backend = backend
        self.model   = None
        self.tokenizer = None
        self.faiss_index = None
        self.faiss_labels: Optional[List[str]] = None
        TEXT_EMBED_CACHE_DIR.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ“ TextEmbedder backend: {self.backend}")

    # ------------------------------------------------------------------
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    # ------------------------------------------------------------------

    def _load_local_model(self):
        """intfloat/multilingual-e5-large ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.model is not None:
            return
        from transformers import AutoTokenizer, AutoModel
        import torch

        print(f"  Loading local model: {TEXT_EMBED_LOCAL_MODEL}")
        self.tokenizer = AutoTokenizer.from_pretrained(TEXT_EMBED_LOCAL_MODEL)
        self.model     = AutoModel.from_pretrained(TEXT_EMBED_LOCAL_MODEL)
        self.model.eval()

        # GPU åˆ©ç”¨å¯èƒ½ãªã‚‰ GPU ã«ç§»å‹•
        import torch
        self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self._device)
        print(f"  Device: {self._device}")

    def _encode_local(self, texts: List[str], batch_size: int) -> np.ndarray:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ (average-pooling + L2-norm)"""
        import torch

        self._load_local_model()
        all_vecs = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            encoded = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors="pt",
            ).to(self._device)

            with torch.no_grad():
                output = self.model(**encoded)
                # average pooling over tokens
                attention_mask = encoded["attention_mask"]
                token_embs = output.last_hidden_state  # (B, T, H)
                mask_exp   = attention_mask.unsqueeze(-1).float()
                vecs = (token_embs * mask_exp).sum(dim=1) / mask_exp.sum(dim=1)
                all_vecs.append(vecs.cpu().numpy())

            if (i // batch_size + 1) % 10 == 0:
                print(f"    {i + len(batch):,} / {len(texts):,} encoded")

        return _normalize(np.vstack(all_vecs).astype(np.float32))

    def _encode_openai(self, texts: List[str], batch_size: int) -> np.ndarray:
        """OpenAI API ã§åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ"""
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError("openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™: pip install openai")

        client  = OpenAI()
        all_vecs = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            resp  = client.embeddings.create(
                model=TEXT_EMBED_OPENAI_MODEL,
                input=batch,
                dimensions=OPENAI_EMBED_DIMENSIONS,
            )
            vecs = np.array([d.embedding for d in resp.data], dtype=np.float32)
            all_vecs.append(vecs)

            if (i // batch_size + 1) % 5 == 0:
                print(f"    {i + len(batch):,} / {len(texts):,} encoded")

        return _normalize(np.vstack(all_vecs))

    # ------------------------------------------------------------------
    # å…¬é–‹ API
    # ------------------------------------------------------------------

    def encode_texts(self, texts: List[str],
                     batch_size: int = TRIPLET_EXTRACT_BATCH_SIZE) -> np.ndarray:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã€‚

        Args:
            texts     : åŸ‹ã‚è¾¼ã‚€ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            batch_size: æ¨è«–ãƒãƒƒãƒã‚µã‚¤ã‚º

        Returns:
            vecs: shape (N, TEXT_EMBED_DIM), float32, L2 æ­£è¦åŒ–æ¸ˆã¿
        """
        if self.backend == "local":
            return self._encode_local(texts, batch_size)
        elif self.backend == "openai":
            return self._encode_openai(texts, batch_size)
        else:
            raise ValueError(f"Unknown backend: {self.backend}")

    def get_embeddings(
        self,
        df: pd.DataFrame,
        equip_col:  str = COLUMNS_V3["equipment_id"],
        sensor_col: str = COLUMNS_V3["check_item_id"],
        cache_path: Optional[Path] = None,
        force_recompute: bool = False,
    ) -> np.ndarray:
        """
        DataFrame ã®å„è¡Œã«ã¤ã„ã¦ (equipment_id, check_item_id) ã‹ã‚‰
        ãƒã‚¹ã‚¿CSVã‚’å¼•ã„ã¦ z âˆˆ â„Â¹â°Â²â´ ã‚’ç”Ÿæˆã€‚

        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (.npz) ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãã‚Œã‚’è¿”ã™ã€‚

        Args:
            df             : training_samples_enriched.csv ãªã©ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            equip_col      : è¨­å‚™IDã‚«ãƒ©ãƒ å (default: 'equipment_id')
            sensor_col     : ãƒã‚§ãƒƒã‚¯é …ç›®IDã‚«ãƒ©ãƒ å (default: 'check_item_id')
            cache_path     : ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å…ˆ (.npz)
            force_recompute: True ã®å ´åˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†è¨ˆç®—

        Returns:
            z: shape (N, TEXT_EMBED_DIM)
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        if cache_path is not None and cache_path.exists() and not force_recompute:
            print(f"  ğŸ“‚ Loading cached embeddings from {cache_path.name}")
            return np.load(cache_path)["embeddings"].astype(np.float32)

        # ãƒã‚¹ã‚¿ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        lookup = load_master_lookup()

        # (equipment_id, check_item_id) â†’ ãƒ†ã‚­ã‚¹ãƒˆ
        print(f"  Building texts for {len(df):,} rows via master lookup ...")
        if equip_col in df.columns and sensor_col in df.columns:
            texts = [
                _build_text_from_ids(row[equip_col], row[sensor_col], lookup)
                for _, row in df[[equip_col, sensor_col]].iterrows()
            ]
        else:
            missing = [c for c in [equip_col, sensor_col] if c not in df.columns]
            print(f"  âš  Columns not found: {missing}. Using placeholder text.")
            texts = ["passage: ç©ºèª¿è¨­å‚™ æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼"] * len(df)

        # åŸ‹ã‚è¾¼ã¿è¨ˆç®—
        print(f"  Encoding {len(texts):,} texts with [{self.backend}] backend ...")
        z = self.encode_texts(texts)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        if cache_path is not None:
            np.savez_compressed(cache_path, embeddings=z)
            print(f"  ğŸ’¾ Saved embedding cache â†’ {cache_path}")

        return z

    # ------------------------------------------------------------------
    # FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    # ------------------------------------------------------------------

    def build_faiss_index(
        self,
        vectors: np.ndarray,
        labels: Optional[List[str]] = None,
        save_path: Path = FAISS_INDEX_PATH,
    ) -> None:
        """
        FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ä¿å­˜ã™ã‚‹ã€‚

        è¨­å‚™ã‚«ãƒ†ã‚´ãƒªã®æ„å‘³ç©ºé–“ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ãŸã‚ã«ä½¿ç”¨ã€‚
        MVP ã§ã¯ near-neighbor æ¤œç´¢ã¯å¿…é ˆã§ã¯ãªã„ãŒã€
        å°†æ¥ã®èª¬æ˜æ€§ (ã©ã®æ­£å¸¸è¨­å‚™ã«æœ€ã‚‚è¿‘ã„ã‹) ã®ãŸã‚ã«æ§‹ç¯‰ã—ã¦ãŠãã€‚

        Args:
            vectors  : (N, D) åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
            labels   : å„ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«ï¼ˆè¨­å‚™åãªã©ï¼‰
            save_path: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜å…ˆ
        """
        try:
            import faiss
        except ImportError:
            print("  âš  faiss not installed. Skipping FAISS index build.")
            print("    Install: pip install faiss-cpu")
            return

        dim = vectors.shape[1]
        vecs = _normalize(vectors)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ â†’ å†…ç©ã§ä»£ç”¨

        print(f"  Building FAISS IVFFlat index: {len(vectors)} vectors, dim={dim}")

        # å°ã‚µã‚¤ã‚ºï¼ˆ< 1000ï¼‰ãªã‚‰ FlatIPã€å¤§ã‚µã‚¤ã‚ºãªã‚‰ IVFFlat
        if len(vectors) < 1000:
            index = faiss.IndexFlatIP(dim)
        else:
            n_centroids = min(64, len(vectors) // 10)
            quantizer   = faiss.IndexFlatIP(dim)
            index       = faiss.IndexIVFFlat(quantizer, dim, n_centroids,
                                              faiss.METRIC_INNER_PRODUCT)
            index.train(vecs)

        if FAISS_USE_GPU:
            try:
                res   = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(res, 0, index)
            except Exception:
                print("  âš  FAISS GPU åˆ©ç”¨ä¸å¯ã€‚CPU ã§ç¶šè¡Œã—ã¾ã™ã€‚")

        index.add(vecs)
        self.faiss_index  = index
        self.faiss_labels = labels or [str(i) for i in range(len(vectors))]

        # ä¿å­˜ (GPU â†’ CPU ã«è»¢æ›ã—ã¦ã‹ã‚‰ä¿å­˜)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        if FAISS_USE_GPU:
            index_cpu = faiss.index_gpu_to_cpu(index)
            faiss.write_index(index_cpu, str(save_path))
        else:
            faiss.write_index(index, str(save_path))

        print(f"  ğŸ’¾ FAISS index saved â†’ {save_path}")

        # labels ã‚‚ä¿å­˜
        labels_path = save_path.with_suffix(".labels.npy")
        np.save(str(labels_path), np.array(self.faiss_labels))
        print(f"  ğŸ’¾ FAISS labels saved â†’ {labels_path}")

    def load_faiss_index(self, index_path: Path = FAISS_INDEX_PATH) -> bool:
        """FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã€‚æˆåŠŸæ™‚ True ã‚’è¿”ã™ã€‚"""
        try:
            import faiss
        except ImportError:
            return False

        if not index_path.exists():
            return False

        self.faiss_index  = faiss.read_index(str(index_path))
        labels_path = index_path.with_suffix(".labels.npy")
        if labels_path.exists():
            self.faiss_labels = list(np.load(str(labels_path), allow_pickle=True))
        print(f"  âœ“ FAISS index loaded: {index_path.name}")
        return True

    def faiss_search(
        self,
        query: np.ndarray,
        k: int = FAISS_N_NEIGHBORS,
    ) -> Tuple[List[str], np.ndarray]:
        """
        FAISS è¿‘å‚æ¢ç´¢ã€‚

        Args:
            query : 1D ã¾ãŸã¯ 2D åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
            k     : è¿”ã™è¿‘å‚æ•°

        Returns:
            (neighbor_labels, distances)
        """
        if self.faiss_index is None:
            raise RuntimeError("FAISS index ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚build_faiss_index() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")

        query_2d = query.reshape(1, -1).astype(np.float32)
        query_2d = _normalize(query_2d)

        distances, indices = self.faiss_index.search(query_2d, k)
        labels = [self.faiss_labels[i] for i in indices[0] if 0 <= i < len(self.faiss_labels)]
        return labels, distances[0]


# =====================================================================
# ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆãƒ»äº‹å‰è¨ˆç®—ç”¨ï¼‰
# =====================================================================

def precompute_all_embeddings(train_df: pd.DataFrame, test_df: pd.DataFrame):
    """
    å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å…¨ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã€‚
    train_triplet_model.py ã®å®Ÿè¡Œå‰ã«ä¸€åº¦ã ã‘å‘¼ã¶ã€‚
    """
    embedder = TextEmbedder()
    lookup   = load_master_lookup()

    print("\n[1/4] Computing train text embeddings ...")
    z_train = embedder.get_embeddings(train_df, cache_path=TEXT_EMBED_TRAIN_NPZ)
    print(f"  âœ“ Train embeddings: {z_train.shape}")

    print("\n[2/4] Computing test text embeddings ...")
    z_test = embedder.get_embeddings(test_df, cache_path=TEXT_EMBED_TEST_NPZ)
    print(f"  âœ“ Test embeddings: {z_test.shape}")

    # ãƒ¦ãƒ‹ãƒ¼ã‚¯ (equipment_id, check_item_id) ã®åŸ‹ã‚è¾¼ã¿ã§ FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
    print("\n[3/4] Building unique equipment embeddings for FAISS ...")
    all_df     = pd.concat([train_df, test_df], ignore_index=True)
    unique_df  = all_df.drop_duplicates(subset=["equipment_id", "check_item_id"])
    z_unique   = embedder.get_embeddings(unique_df, cache_path=TEXT_EMBED_UNIQUE_NPZ)
    unique_labels = (
        unique_df["equipment_id"].astype(str) + "_" + unique_df["check_item_id"].astype(str)
    ).tolist()

    print("\n[4/4] Building FAISS index ...")
    embedder.build_faiss_index(z_unique, labels=unique_labels)

    print("\nâœ… äº‹å‰è¨ˆç®—å®Œäº†ã€‚train_triplet_model.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    return embedder, z_train, z_test


if __name__ == "__main__":
    from config_v3 import PROCESSED_DATA_DIR

    print("="*70)
    print("Text Embedding Pre-computation for v3-0")
    print("="*70)

    train_path = PROCESSED_DATA_DIR / "training_samples_enriched.csv"
    test_path  = PROCESSED_DATA_DIR / "test_samples_enriched.csv"

    if not train_path.exists() or not test_path.exists():
        raise FileNotFoundError(
            "Enriched CSVs not found.\n"
            "å…ˆã« create_enriched_features.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    train_df = pd.read_csv(train_path)
    test_df  = pd.read_csv(test_path)

    precompute_all_embeddings(train_df, test_df)
