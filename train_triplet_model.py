"""
train_triplet_model.py  â€•  MVP v3-0
Triplet Fusion Boosting Classifier for Equipment Anomaly Prediction
è¨­å‚™ç•°å¸¸äºˆæ¸¬ ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆèåˆãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°åˆ†é¡å™¨

â”â”â”â” ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  x âˆˆ â„Â²â¸    çµ±è¨ˆçš„ç‰¹å¾´é‡ï¼ˆcreate_enriched_features.py ã§ç”Ÿæˆï¼‰
  y âˆˆ â„â¶â´    TinyTimeMixer + LoRA åŸ‹ã‚è¾¼ã¿ï¼ˆgranite_ts_model.pyï¼‰
  z âˆˆ â„Â¹â°Â²â´  ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆtext_embedding.pyï¼‰
             â””â”€ è¨­å‚™å Ã— ãƒã‚§ãƒƒã‚¯é …ç›® â†’ multilingual-e5-large
  h = concat(x; y; z)  âˆˆ â„Â¹Â¹Â¹â¶  â†’ LightGBM ã§ {æ­£å¸¸ / ç•°å¸¸} ç¢ºç‡å‡ºåŠ›
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å®Ÿè¡Œæ‰‹é †:
  1. python create_enriched_features.py   # x (çµ±è¨ˆç‰¹å¾´) ç”Ÿæˆ
  2. python text_embedding.py             # z (ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿) ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ
  3. python train_triplet_model.py        # æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆy + çµåˆ + å­¦ç¿’ï¼‰

v2-0 ã¨ã®å·®åˆ†:
  + Text Embedding Path (z) ãŒè¿½åŠ 
  + ç‰¹å¾´é‡æ¬¡å…ƒ: 92 â†’ 1116
  + çµæœä¿å­˜å…ˆ: results/triplet_model/
"""

import sys
import os

# Granite TS ç”¨ã®å›é¿ç­–ï¼štorchvision ã‚’ã‚¹ã‚­ãƒƒãƒ—
sys.modules["torchvision"] = None
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

import torch
from torch.utils.data import Dataset, DataLoader

import lightgbm as lgb
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    confusion_matrix,
    precision_recall_curve,
)
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import seaborn as sns

# v3-0 è¨­å®šï¼ˆv2-0 ã‚’ç¶™æ‰¿ï¼‰
from config_v3 import (
    PROCESSED_DATA_DIR,
    MODEL_ROOT,
    RESULTS_ROOT,
    FORECAST_HORIZONS,
    RANDOM_SEED,
    LOOKBACK_DAYS,
    USE_GPU,
    GPU_ID,
    TRIPLET_MODEL_DIR,
    TRIPLET_RESULTS_DIR,
    TRIPLET_LGBM_PARAMS,
    TRIPLET_NUM_BOOST_ROUND,
    TRIPLET_EARLY_STOPPING,
    TRIPLET_LOG_EVAL_PERIOD,
    STAT_FEATURE_DIM,
    TTM_EMBED_DIM,
    TEXT_Z_DIM,
    TRIPLET_TOTAL_DIM,
    TEXT_EMBED_TRAIN_NPZ,
    TEXT_EMBED_TEST_NPZ,
)

# Text Embedder
from text_embedding import TextEmbedder, precompute_all_embeddings

# Granite TS ãƒ¢ãƒ‡ãƒ«ï¼ˆv2-0 ã¨å…±é€šï¼‰
from granite_ts_model import GraniteTimeSeriesClassifier

# ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
plt.rcParams["font.family"] = "DejaVu Sans"
plt.rcParams["axes.unicode_minus"] = False
sns.set_style("whitegrid")


# =====================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# =====================================================================

class TripletDataset(Dataset):
    """
    Triplet Feature Dataset
    å„ã‚µãƒ³ãƒ—ãƒ«ã« (æ™‚ç³»åˆ—, çµ±è¨ˆç‰¹å¾´, ãƒ©ãƒ™ãƒ«) ã‚’æŒã¤ã€‚
    """

    def __init__(self, df: pd.DataFrame, stat_feature_cols: List[str]):
        self.df = df
        self.stat_feature_cols = stat_feature_cols

        # æ™‚ç³»åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‰
        import ast
        seqs = []
        for seq_str in df["values_sequence"].values:
            try:
                values = ast.literal_eval(str(seq_str))
            except Exception:
                values = [float(x.strip("[] ")) for x in str(seq_str).split(",") if x.strip()]

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° / ãƒˆãƒªãƒŸãƒ³ã‚°
            if len(values) < LOOKBACK_DAYS:
                values = [values[0]] * (LOOKBACK_DAYS - len(values)) + list(values)
            elif len(values) > LOOKBACK_DAYS:
                values = list(values)[-LOOKBACK_DAYS:]
            seqs.append(values)

        self.sequences = np.array(seqs, dtype=np.float32)
        self.stat_feats = df[stat_feature_cols].values.astype(np.float32)
        self.labels = {
            f"label_{h}d": df[f"label_{h}d"].values.astype(np.int64)
            for h in FORECAST_HORIZONS
        }

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        return {
            "sequence": torch.from_numpy(self.sequences[idx].reshape(-1, 1)),
            "stat_feats": torch.from_numpy(self.stat_feats[idx]),
            "labels": {k: v[idx] for k, v in self.labels.items()},
        }


# =====================================================================
# TripletFusionModel
# =====================================================================

class TripletFusionModel:
    """
    Triplet Fusion Boosting Classifier (v3-0)

    ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:
      x = çµ±è¨ˆç‰¹å¾´ (28)  â† create_enriched_features.py
      y = TTM åŸ‹ã‚è¾¼ã¿ (64) â† granite_ts_model.py + LoRA
      z = ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ (1024) â† text_embedding.py
      h = concat(x, y, z) â†’ LightGBM
    """

    def __init__(self):
        self.use_gpu  = USE_GPU and torch.cuda.is_available()
        self.device   = torch.device(f"cuda:{GPU_ID}" if self.use_gpu else "cpu")
        self.ts_encoder: Optional[torch.nn.Module] = None
        self.lgbm_models: Dict[int, lgb.Booster]  = {}
        self.results: Dict[int, dict]              = {}
        self.stat_feature_cols: List[str]          = []

        TRIPLET_MODEL_DIR.mkdir(parents=True, exist_ok=True)
        TRIPLET_RESULTS_DIR.mkdir(parents=True, exist_ok=True)

    # ------------------------------------------------------------------
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    # ------------------------------------------------------------------

    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """enriched CSV ã‚’èª­ã¿è¾¼ã‚€"""
        print("ğŸ“‚ Loading enriched data ...")

        train_path = PROCESSED_DATA_DIR / "training_samples_enriched.csv"
        test_path  = PROCESSED_DATA_DIR / "test_samples_enriched.csv"

        if not train_path.exists() or not test_path.exists():
            raise FileNotFoundError(
                "Enriched CSV ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                "å…ˆã« create_enriched_features.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )

        self.train_df = pd.read_csv(train_path)
        self.test_df  = pd.read_csv(test_path)

        print(f"  âœ“ Train: {len(self.train_df):,} samples")
        print(f"  âœ“ Test : {len(self.test_df):,} samples")

        # çµ±è¨ˆç‰¹å¾´ã‚«ãƒ©ãƒ ã‚’ç‰¹å®šï¼ˆãƒ¡ã‚¿ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ï¼‰
        exclude = {
            "equipment_id", "check_item_id", "date",
            "window_start", "window_end", "values_sequence",
            "label_current", "label_30d", "label_60d", "label_90d",
            "any_anomaly",
        }
        self.stat_feature_cols = [c for c in self.train_df.columns if c not in exclude]
        print(f"  âœ“ Statistical feature cols: {len(self.stat_feature_cols)}")

        return self.train_df, self.test_df

    # ------------------------------------------------------------------
    # 2. ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ (z)
    # ------------------------------------------------------------------

    def load_text_embeddings(self, force_recompute: bool = False) -> Tuple[np.ndarray, np.ndarray]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ text_embedding.py ã‚’ä½¿ã£ã¦ç”Ÿæˆã™ã‚‹ã€‚
        """
        print("\nğŸ”¤ Loading text embeddings (z) ...")

        if (
            not force_recompute
            and TEXT_EMBED_TRAIN_NPZ.exists()
            and TEXT_EMBED_TEST_NPZ.exists()
        ):
            z_train = np.load(TEXT_EMBED_TRAIN_NPZ)["embeddings"].astype(np.float32)
            z_test  = np.load(TEXT_EMBED_TEST_NPZ)["embeddings"].astype(np.float32)
            print(f"  âœ“ Loaded from cache: z_train {z_train.shape}, z_test {z_test.shape}")
        else:
            print("  âš  Cache not found. Computing embeddings now ...")
            print("    (åˆå›ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)")
            _, z_train, z_test = precompute_all_embeddings(self.train_df, self.test_df)

        return z_train, z_test

    # ------------------------------------------------------------------
    # 3. TTM åŸ‹ã‚è¾¼ã¿ (y)
    # ------------------------------------------------------------------

    def build_ts_encoder(self):
        """Granite TS TinyTimeMixer Encoder ã‚’ãƒ­ãƒ¼ãƒ‰"""
        print("\nğŸ¤– Building Granite TS TinyTimeMixer Encoder ...")
        try:
            model = GraniteTimeSeriesClassifier(
                num_horizons=len(FORECAST_HORIZONS),
                device=self.device,
            )
            if hasattr(model, "base_model"):
                self.ts_encoder = model.base_model
            elif hasattr(model, "model"):
                self.ts_encoder = model.model.base_model
            else:
                self.ts_encoder = None

            if self.ts_encoder is not None:
                self.ts_encoder.to(self.device)
                self.ts_encoder.eval()
                print(f"  âœ“ Encoder ready. Device: {self.device}")
            else:
                print("  âš  Encoder å–å¾—ä¸å¯ã€‚ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§ä»£æ›¿ã—ã¾ã™ã€‚")

        except Exception as e:
            print(f"  âš  Encoder ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
            print("    ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã§ä»£æ›¿ã—ã¾ã™ã€‚")
            self.ts_encoder = None

    def extract_ttm_embeddings(
        self,
        df: pd.DataFrame,
        batch_size: int = 256,
    ) -> np.ndarray:
        """TinyTimeMixer ã‹ã‚‰ y âˆˆ â„â¶â´ ã‚’æŠ½å‡ºã™ã‚‹"""
        if self.ts_encoder is None:
            print("  âš  Encoder ãªã— â†’ ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚")
            return np.zeros((len(df), TTM_EMBED_DIM), dtype=np.float32)

        print(f"  Extracting TTM embeddings from {len(df):,} samples ...")
        dataset    = TripletDataset(df, self.stat_feature_cols)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)

        embeddings = []
        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                seqs = batch["sequence"].to(self.device)  # (B, L, 1)
                try:
                    outputs = self.ts_encoder(
                        past_values=seqs,
                        output_hidden_states=True,
                        return_dict=True,
                    )
                    if (
                        hasattr(outputs, "backbone_hidden_state")
                        and outputs.backbone_hidden_state is not None
                    ):
                        bh = outputs.backbone_hidden_state  # (B, 1, P, D)
                        hidden = bh.squeeze(1).mean(dim=1)  # (B, D)
                    else:
                        hidden = seqs.mean(dim=1).squeeze(-1)
                        hidden = hidden.unsqueeze(-1).expand(-1, TTM_EMBED_DIM)
                except Exception:
                    hidden = torch.zeros(seqs.size(0), TTM_EMBED_DIM, device=self.device)

                embeddings.append(hidden.cpu().numpy())

                if (i + 1) % 20 == 0:
                    done = min((i + 1) * batch_size, len(df))
                    print(f"    {done:,} / {len(df):,}")

        y = np.vstack(embeddings).astype(np.float32)
        print(f"  âœ“ TTM embeddings: {y.shape}")
        return y

    # ------------------------------------------------------------------
    # 4. ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç‰¹å¾´ã®çµåˆ h = [x; y; z]
    # ------------------------------------------------------------------

    def prepare_triplet_features(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        x (çµ±è¨ˆ), y (TTM), z (ãƒ†ã‚­ã‚¹ãƒˆ) ã‚’çµåˆã—ã¦ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç‰¹å¾´ã‚’æ§‹ç¯‰ã€‚

        Returns:
            (X_train, X_test)  å„ shape: (N, TRIPLET_TOTAL_DIM)
        """
        print("\nâš¡ Building Triplet Feature h = [x; y; z] ...")

        # x: çµ±è¨ˆç‰¹å¾´
        x_train = self.train_df[self.stat_feature_cols].values.astype(np.float32)
        x_test  = self.test_df[self.stat_feature_cols].values.astype(np.float32)
        print(f"  x (stats)  : train {x_train.shape}, test {x_test.shape}")

        # y: TTM åŸ‹ã‚è¾¼ã¿
        from config_v3 import TTM_EMBED_TRAIN_NPZ, TTM_EMBED_TEST_NPZ
        TTM_EMBED_TRAIN_NPZ.parent.mkdir(parents=True, exist_ok=True)

        if TTM_EMBED_TRAIN_NPZ.exists():
            print("  ğŸ“‚ Loading cached TTM train embeddings ...")
            y_train = np.load(TTM_EMBED_TRAIN_NPZ)["embeddings"].astype(np.float32)
        else:
            y_train = self.extract_ttm_embeddings(self.train_df)
            np.savez_compressed(TTM_EMBED_TRAIN_NPZ, embeddings=y_train)
            print(f"  ğŸ’¾ Saved TTM train cache â†’ {TTM_EMBED_TRAIN_NPZ.name}")

        if TTM_EMBED_TEST_NPZ.exists():
            print("  ğŸ“‚ Loading cached TTM test embeddings  ...")
            y_test = np.load(TTM_EMBED_TEST_NPZ)["embeddings"].astype(np.float32)
        else:
            y_test = self.extract_ttm_embeddings(self.test_df)
            np.savez_compressed(TTM_EMBED_TEST_NPZ, embeddings=y_test)
            print(f"  ğŸ’¾ Saved TTM test  cache â†’ {TTM_EMBED_TEST_NPZ.name}")

        print(f"  y (TTM)    : train {y_train.shape}, test {y_test.shape}")

        # z: ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
        z_train, z_test = self.load_text_embeddings()
        print(f"  z (text)   : train {z_train.shape}, test {z_test.shape}")

        # ã‚µã‚¤ã‚ºæ•´åˆãƒã‚§ãƒƒã‚¯
        for name, x, y, z in [("train", x_train, y_train, z_train),
                               ("test",  x_test,  y_test,  z_test)]:
            n = len(self.train_df) if name == "train" else len(self.test_df)
            assert len(x) == n, f"Stats size mismatch in {name}"
            assert len(y) == n, f"TTM   size mismatch in {name}"
            assert len(z) == n, f"Text  size mismatch in {name}: {len(z)} != {n}"

        # çµåˆ
        self.X_train = np.hstack([x_train, y_train, z_train])
        self.X_test  = np.hstack([x_test,  y_test,  z_test])

        print(f"\n  âœ“ Triplet features ready:")
        print(f"    Train : {self.X_train.shape}  (= {x_train.shape[1]} + {y_train.shape[1]} + {z_train.shape[1]})")
        print(f"    Test  : {self.X_test.shape}")
        print(f"    Total dim: {self.X_train.shape[1]} (expected {TRIPLET_TOTAL_DIM})")

        return self.X_train, self.X_test

    # ------------------------------------------------------------------
    # 5. å­¦ç¿’ï¼ˆå„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼‰
    # ------------------------------------------------------------------

    def _get_lgbm_params(self, pos_weight: float) -> Dict:
        params = dict(TRIPLET_LGBM_PARAMS)
        params["scale_pos_weight"] = pos_weight
        params["random_state"]     = RANDOM_SEED
        return params

    def _build_feature_names(self) -> List[str]:
        """ç‰¹å¾´é‡åãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        names = list(self.stat_feature_cols)
        names += [f"ttm_{i}" for i in range(TTM_EMBED_DIM)]
        names += [f"text_{i}" for i in range(TEXT_Z_DIM)]
        return names

    def train_horizon(self, horizon: int) -> Tuple[lgb.Booster, Dict]:
        """ç‰¹å®šãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã® LightGBM ã‚’å­¦ç¿’"""
        print(f"\n{'='*70}")
        print(f"  Training Triplet Fusion Classifier â€” {horizon}d horizon")
        print(f"{'='*70}")

        label_col = f"label_{horizon}d"
        y_train   = self.train_df[label_col].values
        y_test    = self.test_df[label_col].values

        pos_rate = y_train.mean()
        pos_weight = (1 - pos_rate) / pos_rate if pos_rate > 0 else 1.0

        print(f"  Train pos: {y_train.sum():,} / {len(y_train):,}  ({pos_rate*100:.1f}%)")
        print(f"  Test  pos: {y_test.sum():,}  / {len(y_test):,}   ({y_test.mean()*100:.1f}%)")
        print(f"  pos_weight: {pos_weight:.2f}")

        feature_names = self._build_feature_names()
        params = self._get_lgbm_params(pos_weight)

        train_data = lgb.Dataset(self.X_train, label=y_train, feature_name=feature_names)
        test_data  = lgb.Dataset(self.X_test,  label=y_test,  reference=train_data,
                                 feature_name=feature_names)

        print(f"\n  ğŸš€ Training LightGBM ...")
        model = lgb.train(
            params,
            train_data,
            num_boost_round=TRIPLET_NUM_BOOST_ROUND,
            valid_sets=[train_data, test_data],
            valid_names=["train", "test"],
            callbacks=[
                lgb.early_stopping(stopping_rounds=TRIPLET_EARLY_STOPPING),
                lgb.log_evaluation(period=TRIPLET_LOG_EVAL_PERIOD),
            ],
        )

        self.lgbm_models[horizon] = model

        # è©•ä¾¡
        y_pred_proba = model.predict(self.X_test, num_iteration=model.best_iteration)

        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)
        f1_scores = (
            2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)
        )
        opt_idx   = np.argmax(f1_scores)
        opt_thr   = thresholds[opt_idx]
        y_pred    = (y_pred_proba > opt_thr).astype(int)

        metrics = {
            "horizon":           horizon,
            "model":             "TripletFusion",
            "optimal_threshold": float(opt_thr),
            "accuracy":          float(accuracy_score(y_test, y_pred)),
            "precision":         float(precision_score(y_test, y_pred, zero_division=0)),
            "recall":            float(recall_score(y_test, y_pred, zero_division=0)),
            "f1":                float(f1_score(y_test, y_pred, zero_division=0)),
            "roc_auc":           float(roc_auc_score(y_test, y_pred_proba)),
            "pr_auc":            float(average_precision_score(y_test, y_pred_proba)),
            "best_iteration":    model.best_iteration,
            "triplet_dim":       self.X_train.shape[1],
        }

        self.results[horizon] = {
            "metrics":     metrics,
            "predictions": y_pred_proba,
            "labels":      y_test,
        }

        print(f"\n  ğŸ“Š {horizon}d Results:")
        print(f"    Threshold : {opt_thr:.4f}")
        print(f"    Precision : {metrics['precision']:.4f}")
        print(f"    Recall    : {metrics['recall']:.4f}")
        print(f"    F1-Score  : {metrics['f1']:.4f}")
        print(f"    ROC-AUC   : {metrics['roc_auc']:.4f}")
        print(f"    PR-AUC    : {metrics['pr_auc']:.4f}")

        return model, metrics

    def train_all_horizons(self) -> pd.DataFrame:
        """å…¨ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®å­¦ç¿’"""
        print("\n" + "="*70)
        print("ğŸš€ Triplet Fusion Boosting Classifier â€” Training All Horizons")
        print("="*70)
        print(f"    ç‰¹å¾´æ¬¡å…ƒ h = [x:{STAT_FEATURE_DIM} | y:{TTM_EMBED_DIM} | z:{TEXT_Z_DIM}]")

        all_metrics = []
        for h in FORECAST_HORIZONS:
            _, m = self.train_horizon(h)
            all_metrics.append(m)

        metrics_df = pd.DataFrame(all_metrics)
        print("\n" + "="*70)
        print("ğŸ“Š Summary â€” Triplet Fusion Classifier")
        print("="*70)
        print(metrics_df[["horizon", "precision", "recall", "f1", "roc_auc"]].to_string(index=False))

        return metrics_df

    # ------------------------------------------------------------------
    # 6. ä¿å­˜
    # ------------------------------------------------------------------

    def save_models(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨è©•ä¾¡çµæœã‚’ä¿å­˜"""
        print(f"\nğŸ’¾ Saving models â†’ {TRIPLET_MODEL_DIR}")

        for h, model in self.lgbm_models.items():
            path = TRIPLET_MODEL_DIR / f"lgbm_triplet_{h}d.txt"
            model.save_model(str(path))
            print(f"  âœ“ {h}d model: {path.name}")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ CSV
        rows  = [self.results[h]["metrics"] for h in FORECAST_HORIZONS if h in self.results]
        m_df  = pd.DataFrame(rows)
        m_path = TRIPLET_RESULTS_DIR / "metrics_summary.csv"
        m_df.to_csv(m_path, index=False, encoding="utf-8-sig")
        print(f"  âœ“ Metrics: {m_path}")

        # JSON å½¢å¼ã§ã‚‚ä¿å­˜
        j_path = TRIPLET_RESULTS_DIR / "metrics_summary.json"
        with open(j_path, "w", encoding="utf-8") as f:
            json.dump(rows, f, ensure_ascii=False, indent=2)
        print(f"  âœ“ JSON: {j_path}")

    # ------------------------------------------------------------------
    # 7. å¯è¦–åŒ–
    # ------------------------------------------------------------------

    def plot_results(self):
        """ç‰¹å¾´é‡é‡è¦åº¦ Top50 ã‚’å¯è¦–åŒ–ï¼ˆhorizon ã”ã¨ã«æ¨ªä¸¦ã³ï¼‰"""
        import matplotlib.patches as mpatches
        print(f"\nPlotting results -> {TRIPLET_RESULTS_DIR}")

        TOP_N = 50
        BAR_H = 0.55
        n_horizons = len(FORECAST_HORIZONS)
        feature_names = self._build_feature_names()

        fig_h = TOP_N * BAR_H * 0.14 + 2.5
        fig, axes = plt.subplots(1, n_horizons, figsize=(8 * n_horizons, fig_h))
        if n_horizons == 1:
            axes = [axes]

        legend_handles = [
            mpatches.Patch(color="#4CAF50", label="Text embedding"),
            mpatches.Patch(color="#2196F3", label="TTM embedding"),
            mpatches.Patch(color="#FF9800", label="Statistical feature"),
        ]

        for col_idx, h in enumerate(FORECAST_HORIZONS):
            ax    = axes[col_idx]
            model = self.lgbm_models.get(h)
            if model is None or h not in self.results:
                ax.axis("off")
                continue

            importance = model.feature_importance(importance_type="gain")
            top_idx    = np.argsort(importance)[-TOP_N:][::-1]
            top_names  = [feature_names[i] if i < len(feature_names) else f"feat_{i}"
                          for i in top_idx]
            top_vals   = importance[top_idx]

            colors = []
            for nm in top_names:
                if nm.startswith("text_"):
                    colors.append("#4CAF50")   # green : text embedding
                elif nm.startswith("ttm_"):
                    colors.append("#2196F3")   # blue  : TTM embedding
                else:
                    colors.append("#FF9800")   # orange: statistical feature

            ax.barh(range(TOP_N), top_vals[::-1], color=colors[::-1], height=BAR_H)
            ax.set_yticks(range(TOP_N))
            ax.set_yticklabels(top_names[::-1], fontsize=7)
            ax.set_ylim(-0.5, TOP_N - 0.5)   # ä¸Šä¸‹ã®éš™é–“ã‚’è©°ã‚ã‚‹

            m = self.results[h]["metrics"]
            ax.set_title(
                f"{h}d Feature Importance Top{TOP_N}\n"
                f"Prec={m['precision']:.3f}  Rec={m['recall']:.3f}  "
                f"F1={m['f1']:.3f}  AUC={m['roc_auc']:.4f}",
                fontsize=10,
            )
            ax.set_xlabel("Gain")
            # å„ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®å³ä¸‹ã«å‡¡ä¾‹
            ax.legend(handles=legend_handles, loc="lower right", fontsize=8, framealpha=0.9)

        plt.suptitle("Triplet Fusion Boosting Classifier (v3-0) - Feature Importance",
                     fontsize=13, y=1.005)
        plt.tight_layout()

        fig_path = TRIPLET_RESULTS_DIR / "triplet_model_evaluation.png"
        plt.savefig(fig_path, dpi=150, bbox_inches="tight")
        plt.close()
        print(f"  Feature importance figure saved: {fig_path}")

    # ------------------------------------------------------------------
    # 8. v2-0 ã¨ã®æ¯”è¼ƒ
    # ------------------------------------------------------------------

    def compare_with_v2(self) -> pd.DataFrame:
        """v2-0ï¼ˆHybridï¼‰ã®çµæœã¨æ¯”è¼ƒ"""
        print(f"\nğŸ“Š Comparing v3-0 vs v2-0 ...")

        rows = []

        # v2-0 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®çµæœã‚’èª­ã¿è¾¼ã¿
        v2_path = RESULTS_ROOT / "hybrid_model" / "metrics_summary.csv"
        if v2_path.exists():
            v2_df = pd.read_csv(v2_path)
            for _, r in v2_df.iterrows():
                rows.append({
                    "Model":     "v2-0 Hybrid (92-dim)",
                    "Horizon":   f"{int(r['horizon'])}d",
                    "Precision": r.get("precision", float("nan")),
                    "Recall":    r.get("recall", float("nan")),
                    "F1":        r.get("f1", float("nan")),
                    "ROC-AUC":   r.get("roc_auc", float("nan")),
                })
        else:
            print("  âš  v2-0 metrics not found. Skipping comparison.")

        # v3-0 ã®çµæœ
        for h in FORECAST_HORIZONS:
            if h not in self.results:
                continue
            m = self.results[h]["metrics"]
            rows.append({
                "Model":     f"v3-0 Triplet (1116-dim)",
                "Horizon":   f"{h}d",
                "Precision": m["precision"],
                "Recall":    m["recall"],
                "F1":        m["f1"],
                "ROC-AUC":   m["roc_auc"],
            })

        comparison_df = pd.DataFrame(rows)
        cmp_path = TRIPLET_RESULTS_DIR / "comparison_v2_vs_v3.csv"
        comparison_df.to_csv(cmp_path, index=False, encoding="utf-8-sig")

        print("\n" + "="*70)
        print("Model Comparison: v2-0 Hybrid vs v3-0 Triplet Fusion")
        print("="*70)
        print(comparison_df.to_string(index=False))
        print(f"\n  ğŸ’¾ Saved: {cmp_path}")
        return comparison_df


# =====================================================================
# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# =====================================================================

def main():
    print("=" * 70)
    print("MVP v3-0: Triplet Feature Learning for Equipment Anomaly Prediction")
    print("  Architecture: x(28) + y(64) + z(1024) â†’ LightGBM")
    print("=" * 70)
    print(f"  Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    model_obj = TripletFusionModel()

    # Step 1: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    model_obj.load_data()

    # Step 2: Granite TS Encoder ãƒ­ãƒ¼ãƒ‰ï¼ˆy ã®æŠ½å‡ºã«ä½¿ç”¨ï¼‰
    model_obj.build_ts_encoder()

    # Step 3: ãƒˆãƒªãƒ—ãƒ¬ãƒƒãƒˆç‰¹å¾´ã®æ§‹ç¯‰ [x; y; z]
    model_obj.prepare_triplet_features()

    # Step 4: å…¨ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã§å­¦ç¿’
    metrics_df = model_obj.train_all_horizons()

    # Step 5: ä¿å­˜
    model_obj.save_models()

    # Step 6: å¯è¦–åŒ–
    model_obj.plot_results()

    # Step 7: v2-0 ã¨ã®æ¯”è¼ƒ
    model_obj.compare_with_v2()

    print("\n" + "=" * 70)
    print(f"âœ… v3-0 Training Completed! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print(f"  Models  â†’ {TRIPLET_MODEL_DIR}")
    print(f"  Results â†’ {TRIPLET_RESULTS_DIR}")


if __name__ == "__main__":
    main()
